{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a750d8",
   "metadata": {},
   "source": [
    "Kiểm tra DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80e80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29000/29000 [00:00<00:00, 43987.55it/s]\n",
      "100%|██████████| 29000/29000 [00:00<00:00, 37663.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 29000\n",
      "Vocab EN size: 5893\n",
      "Vocab FR size: 6470\n",
      "SRC shape: torch.Size([4, 17])\n",
      "TRG shape: torch.Size([4, 18])\n",
      "SRC example: tensor([   1,   16,   24,   15,   25,  774,   17,   57,   80,  202, 1305,    5,\n",
      "           2,    0,    0,    0,    0])\n",
      "TRG example: tensor([   1,   21,   81,   32,  214,   28,   88,   70,    7, 1171,    5,    2,\n",
      "           0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.data import get_loader, load_vocab, load_dataset\n",
    "\n",
    "# Load DataLoader\n",
    "train_loader, train_dataset = get_loader(\"data/train.en.gz\", \"data/train.fr.gz\", batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Vocab EN size: {len(train_dataset.src_vocab.stoi)}\")\n",
    "print(f\"Vocab FR size: {len(train_dataset.trg_vocab.stoi)}\")\n",
    "\n",
    "# Kiểm tra batch đầu tiên\n",
    "for src, trg in train_loader:\n",
    "    print(\"SRC shape:\", src.shape)  # [batch_size, seq_len]\n",
    "    print(\"TRG shape:\", trg.shape)\n",
    "    print(\"SRC example:\", src[0])\n",
    "    print(\"TRG example:\", trg[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a83f7",
   "metadata": {},
   "source": [
    "Test việc lưu và load file vocab, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e25cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29000/29000 [00:00<00:00, 85951.59it/s] \n",
      "100%|██████████| 29000/29000 [00:00<00:00, 61477.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocab và dataset đã lưu xong.\n"
     ]
    }
   ],
   "source": [
    "from src.data import get_loader, save_vocab, save_dataset\n",
    "\n",
    "# Load DataLoader + Dataset\n",
    "train_loader, train_dataset = get_loader(\"data/train.en.gz\", \"data/train.fr.gz\", batch_size=32)\n",
    "\n",
    "# Lưu vocab và dataset\n",
    "save_vocab(train_dataset.src_vocab, \"data/vocab_en.pkl\")\n",
    "save_vocab(train_dataset.trg_vocab, \"data/vocab_fr.pkl\")\n",
    "save_dataset(train_dataset, \"data/train_dataset.pt\")\n",
    "\n",
    "print(\"✅ Vocab và dataset đã lưu xong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31e262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded EN vocab size: 5893\n",
      "Loaded FR vocab size: 6470\n"
     ]
    }
   ],
   "source": [
    "from src.data import load_vocab, load_dataset\n",
    "\n",
    "# Load lại vocab\n",
    "src_vocab = load_vocab(\"data/vocab_en.pkl\")\n",
    "trg_vocab = load_vocab(\"data/vocab_fr.pkl\")\n",
    "\n",
    "print(\"Loaded EN vocab size:\", len(src_vocab.stoi))\n",
    "print(\"Loaded FR vocab size:\", len(trg_vocab.stoi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88867936",
   "metadata": {},
   "source": [
    "Xem File vocab_en.pkl / vocab_fr.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4019a92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocab size: 5893\n",
      "FR vocab size: 6470\n",
      "EN sample: [('<pad>', 0), ('<sos>', 1), ('<eos>', 2), ('<unk>', 3), ('a', 4), ('.', 5), ('in', 6), ('the', 7), ('on', 8), ('man', 9)]\n",
      "FR sample: [('<pad>', 0), ('<sos>', 1), ('<eos>', 2), ('<unk>', 3), ('un', 4), ('.', 5), ('une', 6), ('de', 7), ('en', 8), (\"d'\", 9)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load vocab\n",
    "with open(\"data/vocab_en.pkl\", \"rb\") as f:\n",
    "    src_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"data/vocab_fr.pkl\", \"rb\") as f:\n",
    "    trg_vocab = pickle.load(f)\n",
    "\n",
    "# Xem tổng số từ\n",
    "print(\"EN vocab size:\", len(src_vocab.stoi))\n",
    "print(\"FR vocab size:\", len(trg_vocab.stoi))\n",
    "\n",
    "# In 10 từ đầu tiên trong vocab\n",
    "print(\"EN sample:\", list(src_vocab.stoi.items())[:10])\n",
    "print(\"FR sample:\", list(trg_vocab.stoi.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28925c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã xuất vocab ra file: data/vocab_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load vocab tiếng Anh\n",
    "with open(\"data/vocab_en.pkl\", \"rb\") as f:\n",
    "    src_vocab = pickle.load(f)\n",
    "\n",
    "# Load vocab tiếng Pháp\n",
    "with open(\"data/vocab_fr.pkl\", \"rb\") as f:\n",
    "    trg_vocab = pickle.load(f)\n",
    "\n",
    "# Tạo DataFrame cho vocab EN\n",
    "df_en = pd.DataFrame(list(src_vocab.stoi.items()), columns=[\"token\", \"index\"])\n",
    "\n",
    "# Tạo DataFrame cho vocab FR\n",
    "df_fr = pd.DataFrame(list(trg_vocab.stoi.items()), columns=[\"token\", \"index\"])\n",
    "\n",
    "# Ghi ra Excel (2 sheet)\n",
    "with pd.ExcelWriter(\"data/vocab_all.xlsx\") as writer:\n",
    "    df_en.to_excel(writer, sheet_name=\"English\", index=False)\n",
    "    df_fr.to_excel(writer, sheet_name=\"French\", index=False)\n",
    "\n",
    "print(\"✅ Đã xuất vocab ra file: data/vocab_all.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c75542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "EN: <sos> two young , white males are outside near many bushes . <eos>\n",
      "FR: <sos> deux jeunes hommes blancs sont dehors près de buissons . <eos>\n",
      "---\n",
      "Sample 2:\n",
      "EN: <sos> several men in hard hats are operating a giant pulley system . <eos>\n",
      "FR: <sos> plusieurs hommes en casque font fonctionner un système de poulies géant . <eos>\n",
      "---\n",
      "Sample 3:\n",
      "EN: <sos> a little girl climbing into a wooden playhouse . <eos>\n",
      "FR: <sos> une petite fille grimpe dans une maisonnette en bois . <eos>\n",
      "---\n",
      "Sample 4:\n",
      "EN: <sos> a man in a blue shirt is standing on a ladder cleaning a window . <eos>\n",
      "FR: <sos> un homme dans une chemise bleue se tient sur une échelle pour nettoyer une fenêtre . <eos>\n",
      "---\n",
      "Sample 5:\n",
      "EN: <sos> two men are at the stove preparing food . <eos>\n",
      "FR: <sos> deux hommes aux fourneaux préparent à manger . <eos>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 5  # số câu muốn xem\n",
    "for i in range(NUM_SAMPLES):\n",
    "    src_idx, trg_idx = train_dataset[i]\n",
    "\n",
    "    # Chuyển index -> từ\n",
    "    src_words = [src_vocab.itos[idx.item()] for idx in src_idx if idx.item() not in [src_vocab.stoi[\"<pad>\"]]]\n",
    "    trg_words = [trg_vocab.itos[idx.item()] for idx in trg_idx if idx.item() not in [trg_vocab.stoi[\"<pad>\"]]]\n",
    "\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"EN:\", \" \".join(src_words))\n",
    "    print(\"FR:\", \" \".join(trg_words))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459df4a6",
   "metadata": {},
   "source": [
    "2️⃣ Kiểm tra vocab và dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907b53d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source indices: tensor([   1,   16,   24,   15,   25,  774,   17,   57,   80,  202, 1305,    5,\n",
      "           2])\n",
      "Target indices: tensor([   1,   21,   81,   32,  214,   28,   88,   70,    7, 1171,    5,    2])\n"
     ]
    }
   ],
   "source": [
    "src_example, trg_example = train_dataset[0]\n",
    "print(\"Source indices:\", src_example)\n",
    "print(\"Target indices:\", trg_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c0dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: <sos> two young , white males are outside near many bushes . <eos>\n",
      "Target sentence: <sos> deux jeunes hommes blancs sont dehors près de buissons . <eos>\n"
     ]
    }
   ],
   "source": [
    "src_words = [src_vocab.itos[idx.item()] for idx in src_example]\n",
    "trg_words = [trg_vocab.itos[idx.item()] for idx in trg_example]\n",
    "print(\"Source sentence:\", \" \".join(src_words))\n",
    "print(\"Target sentence:\", \" \".join(trg_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9f863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source indices: tensor([   1,   16,   24,   15,   25,  774,   17,   57,   80,  202, 1305,    5,\n",
      "           2])\n",
      "Target indices: tensor([   1,   21,   81,   32,  214,   28,   88,   70,    7, 1171,    5,    2])\n",
      "Source sentence: <sos> two young , white males are outside near many bushes . <eos>\n",
      "Target sentence: <sos> deux jeunes hommes blancs sont dehors près de buissons . <eos>\n"
     ]
    }
   ],
   "source": [
    "# Lấy một sample đầu tiên từ dataset\n",
    "src_example, trg_example = train_dataset[0]\n",
    "\n",
    "# In ra các index (số) trong câu\n",
    "print(\"Source indices:\", src_example)\n",
    "print(\"Target indices:\", trg_example)\n",
    "\n",
    "# Chuyển các index về từ để đọc được\n",
    "src_words = [src_vocab.itos[idx.item()] for idx in src_example]\n",
    "trg_words = [trg_vocab.itos[idx.item()] for idx in trg_example]\n",
    "\n",
    "print(\"Source sentence:\", \" \".join(src_words))\n",
    "print(\"Target sentence:\", \" \".join(trg_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee3dd3",
   "metadata": {},
   "source": [
    "3️⃣ Kiểm tra DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150e780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24]) torch.Size([32, 25])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.data import load_dataset, MyCollate, load_vocab\n",
    "\n",
    "# Load dataset + vocab\n",
    "train_dataset = load_dataset(\"data/train_dataset.pt\")\n",
    "src_vocab = load_vocab(\"data/vocab_en.pkl\")\n",
    "trg_vocab = load_vocab(\"data/vocab_fr.pkl\")\n",
    "\n",
    "PAD_IDX = src_vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tạo DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=MyCollate(PAD_IDX))\n",
    "\n",
    "# Xem shape batch đầu tiên\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(src_batch.shape, trg_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b6816",
   "metadata": {},
   "source": [
    "src_batch.shape = [32, 23] → batch size = 32, sequence length nguồn = 23 token.\n",
    "\n",
    "trg_batch.shape = [32, 25] → batch size = 32, sequence length đích = 25 token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a23c9",
   "metadata": {},
   "source": [
    "4️⃣ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9492be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 4.3699\n"
     ]
    }
   ],
   "source": [
    "%run src/train_data.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        output = model(src, trg, teacher_forcing_ratio=0)\n",
    "        top_words = output.argmax(-1)\n",
    "        print(\"Src:\", src[0])\n",
    "        print(\"Pred:\", top_words[0])\n",
    "        print(\"Trg:\", trg[0])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeccd6f",
   "metadata": {},
   "source": [
    "5️⃣ Kiểm tra model sau training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22588e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/seq2seq_epoch10.pth\"))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    src_tensor = src_example.unsqueeze(0).to(DEVICE)\n",
    "    trg_tensor = trg_example.unsqueeze(0).to(DEVICE)\n",
    "    output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "    pred_indices = output.argmax(-1)[0].cpu().tolist()\n",
    "    pred_sentence = [trg_vocab.itos[idx] for idx in pred_indices]\n",
    "    print(\"Predicted sentence:\", \" \".join(pred_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97549394",
   "metadata": {},
   "source": [
    "6️⃣ Gợi ý cải tiến\n",
    "\n",
    "1. Teacher forcing: hiện tại là cố định 0.5. Bạn có thể giảm dần theo epoch\n",
    "\n",
    "2. Gradient clipping: tránh exploding gradients với LSTM:\n",
    "\n",
    "3. Validation set: nếu có dataset validation, theo dõi val_loss sẽ tốt hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2356b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = max(0.5 * (0.9 ** epoch), 0.1)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb4882",
   "metadata": {},
   "source": [
    "Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48560f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/seq2seq_epoch10.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "model.eval()  # Chuyển model sang chế độ eval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
